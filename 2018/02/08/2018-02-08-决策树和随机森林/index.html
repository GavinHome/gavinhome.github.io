<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><title>决策树与随机森林 | GavinHome Blog</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="gavinhome 沐渊 yangxiaomin gavinhome.github.io muyuan"><meta name="description" content="风起青萍之末, 浪成微澜之间"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="http://gavinhome.github.io/2018/02/08/2018-02-08-决策树和随机森林/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="Gavinhome Blog"><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/img/loader.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="Gavinhome Blog" alt="Gavinhome Blog"><img src="/img/logo-text-white.png" alt="Gavinhome Blog"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/article" alt="文章" title="文章">文章</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/img/static/2018-01-28/figure_1.png" alt="决策树与随机森林"></div><header class="post__info"><h1 class="post__title">决策树与随机森林</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://www.github.com/gavinhome">gavinhome</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2018-02-08</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/机器学习/">机器学习</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><h2 id="信息熵与信息增益"><a href="#信息熵与信息增益" class="headerlink" title="信息熵与信息增益"></a>信息熵与信息增益</h2><p>决策树是一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶结点代表一种类别；决策树学习以实例为基础的归纳学习算法，采用自顶向下的递归方法，基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶子结点中的实例都是同一类。</p><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>当熵和条件熵中的概率由数据估计（特极大似然估计）得到时，所对应的<br>熵和条件熵成为经验熵和经验条件熵。</p><p>信息增益表示得知特征A的信息而使得类X的不确定性减少的程度。</p><p>定义：特征A对训练集D的信息增益$g\left(D,A\right)$, 定义为集合D的经验熵$H\left(D\right)$与特征A给定条件下D的经验条件熵</p><p>$H \left( D|A \right)$<br>之差，即：</p><p>$$g\left(D,A\right) = H\left(D\right) - H\left(D|A\right)$$</p><p>显然，这即为训练数据集D和特征A的互信息</p><h3 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h3><h3 id="基本记号"><a href="#基本记号" class="headerlink" title="基本记号"></a>基本记号</h3><ol><li><p>设训练集D,<br>$\left| D \right|$ 表示样本个数。</p></li><li><p>设K个类 $C_k$,k=1,2,…K,<br>$\left|C_k\right|$为属于类$C_k$<br>的样本个数，有: $\sum\left|C_k\right| = |D|$</p></li><li><p>设特征A有n个不同的取值${a_{1},a_{2}…a_{n}}$,<br>根据特征A的取值将D划分为n个子集$D_{1},D_{2}…D_{n}$,$\left|D_i\right|$为$D_i$<br>的样本个数，$\sum\left|D_i\right| = \left|D\right|$</p></li><li><p>设子集$\left|D_i\right|$<br>中属于类$C_k$<br>的样本集合为$\left|D_{ik}\right|$, $\left|D_{ik}\right|$为$D_{ik}$的样本个数</p></li></ol><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>建立决策树的关键是选择哪个属性作为分类依据，根据不同的目标函数，共有三种方法来：</p><ol><li><p>ID3: 使用信息增益/互信息进行特征选择。取值多的属性，更容易使数据更纯，其信息增益更大。</p></li><li><p>C4.5: 信息增益率：</p></li><li><p>CART: Gini系数</p></li></ol><p>一个属性的信息增益越大，表明属性对样本的熵减少的能力越强，这个属性使得数据由不确定性变为确定性的能力越强。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>$$C\left(T\right) = \sum_{} N_{t}H\left(t\right)$$</p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>决策树与训练数据有很好的分类问题，但对未知的册书数据未必有好的分类能力，泛化能力弱，即可能发生过拟合现象。</p><h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><p>总体思路是：由完全树T0开始，剪枝部分结点得到T1，再次剪枝部分结点得到T2…直到仅剩树根的树Tk;在验证数据集上对k个树分别评价，选择损失函数最小的树Ta.三种决策树的过程相同，却别是评价标准不同：信息增益，信息增益率，基尼系数。</p><p>剪枝算法：<br>给定决策时T0</p><ol><li>计算所有内部节点的剪枝系数</li><li>查找最小剪枝系数的节点，剪枝的决策树TK</li><li>重复以上步骤，知道决策树TK只有一个节点</li><li>得到T0T1T2…TK</li><li>使用验证眼样本集选择最优子树, 标准为平价函数(损失函数):</li></ol><p>$$C\left(T\right) = \sum_{} N_{t}H\left(t\right)$$</p><h2 id="Bagging与随机森林"><a href="#Bagging与随机森林" class="headerlink" title="Bagging与随机森林"></a>Bagging与随机森林</h2><h3 id="Bagging策略"><a href="#Bagging策略" class="headerlink" title="Bagging策略"></a>Bagging策略</h3><ol><li>bootstrap aggregation</li><li>从样本集中重采样选出n个样本</li><li>在所有属性上，对n个样本建立分类器</li><li>重复以上两步m次，获得了m个分类器</li><li>将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类</li></ol><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><ol><li>从样本集中用bootstrap采样选出n个样本</li><li>从所有属性中随机选择K个属性，选择最佳分割属性作为节点建立CART决策树</li><li>重复以上两步m次，建立m棵CART决策树</li><li>这m个CART形成随机森林，通过投票表决结果</li></ol><p>可以使用决策树，svm或逻辑回归等，总称为“随机森林”</p><h3 id="投票机制"><a href="#投票机制" class="headerlink" title="投票机制"></a>投票机制</h3><ol><li><p>简单投票：一票否决；少数服从多数，有效多数（加权）；阈值表决</p></li><li><p>贝叶斯投票</p></li></ol><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h4 id="决策树-1"><a href="#决策树-1" class="headerlink" title="决策树"></a>决策树</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span>  datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris, load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">x = X[:,:<span class="number">2</span>]</span><br><span class="line">x__train, x__test, y__train, y__test = train_test_split(x, y, test_size=<span class="number">0.33</span>)</span><br><span class="line">dt_clf = tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, min_samples_leaf=<span class="number">3</span>)</span><br><span class="line">dt_clf = dt_clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">N, M = <span class="number">500</span>, <span class="number">500</span></span><br><span class="line">x1_min, x1_max = x[:,<span class="number">0</span>].min(),x[:,<span class="number">0</span>].max()</span><br><span class="line">x2_min, x2_max = x[:,<span class="number">1</span>].min(),x[:,<span class="number">1</span>].max()</span><br><span class="line">t1 = np.linspace(x1_min,x1_max,N)</span><br><span class="line">t2 = np.linspace(x2_min,x2_max,M)</span><br><span class="line">x1,x2 = np.meshgrid(t1, t2)</span><br><span class="line">x___test = np.stack((x1.flat,x2.flat), axis=<span class="number">1</span>)</span><br><span class="line">y_hat = dt_clf.predict(x___test)</span><br><span class="line">y_hat = y_hat.reshape(x1.shape)</span><br><span class="line">plt.pcolormesh(x1,x2,y_hat,cmap=plt.cm.prism)</span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>], c=y, edgecolors=<span class="string">'k'</span>,cmap=plt.cm.prism)</span><br><span class="line">plt.xlabel(<span class="string">'Sepal Length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal Width'</span>)</span><br><span class="line">plt.xlim(x1_min, x1_max)</span><br><span class="line">plt.ylim(x2_min, x2_max)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>)</span><br><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, min_samples_leaf=<span class="number">3</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">f = open(<span class="string">"iris_tree.dot"</span>,<span class="string">'w'</span>)</span><br><span class="line">tree.export_graphviz(clf, out_file=f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluating accuracy</span></span><br><span class="line">accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)</span><br><span class="line">print(<span class="string">'Accuracy : &#123;&#125;'</span>.format(accuracy))</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="/img/static/2018-01-28/figure_1.png" alt="png"></p><p>分类准确率为：0.94</p><h4 id="随机森林-1"><a href="#随机森林-1" class="headerlink" title="随机森林"></a>随机森林</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span>  datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris, load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl <span class="comment"># 设置环境变量</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 绘图专用</span></span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>)</span><br><span class="line">clf = ensemble.RandomForestClassifier(criterion=<span class="string">'entropy'</span>, min_samples_leaf=<span class="number">2</span>, max_depth=<span class="number">10</span>,min_samples_split=<span class="number">3</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluating accuracy</span></span><br><span class="line">accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)</span><br><span class="line">print(<span class="string">'Accuracy : &#123;&#125;'</span>.format(accuracy))</span><br></pre></td></tr></table></figure><p>分类结果为：0.96</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>决策树与随机森林使用简单，可以作为对数据分布探索的首要尝试算法。</li><li>随机森林的集成思想也可用于其他分类器的设计中。如果通过随机森林做样本异常值检测，则总计样本间位于相同决策树的叶节点的个数，形成样本相似度矩阵。</li></ol><div class="post-announce">感谢您的阅读，本文由 <a href="http://gavinhome.github.io">Gavinhome Blog</a> 版权所有。如若转载，请注明出处：Gavinhome Blog（<a href="http://gavinhome.github.io/2018/02/08/2018-02-08-决策树和随机森林/">http://gavinhome.github.io/2018/02/08/2018-02-08-决策树和随机森林/</a>）</div><div class="post__prevs"><div class="post__prev"><a href="/2018/01/27/2018-01-27-支持向量机/" title="支持向量机（SVM）"><i class="iconfont icon-prev"></i>支持向量机（SVM）</a></div><div class="post__prev post__prev--right"><a href="/2019/03/29/hello-world/" title="Hello World">Hello World<i class="iconfont icon-next"></i></a></div></div></div></article><div id="comment-container"></div></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">风起青萍之末, 浪成微澜之间</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/机器学习/">机器学习</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/hexo教程/">hexo教程</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Python/">Python</a><span class="block-list-count">1</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2019/03/29/hello-world/" title="Hello World"><div class="item__cover"><img src="/img/default_cover.png" alt="Hello World"></div><div class="item__info"><h3 class="item__title">Hello World</h3><span class="item__text">2019-03-29</span></div></a></li><li class="latest-post-item"><a href="/2018/02/08/2018-02-08-决策树和随机森林/" title="决策树与随机森林"><div class="item__cover"><img src="/img/static/2018-01-28/figure_1.png" alt="决策树与随机森林"></div><div class="item__info"><h3 class="item__title">决策树与随机森林</h3><span class="item__text">2018-02-08</span></div></a></li><li class="latest-post-item"><a href="/2018/01/27/2018-01-27-支持向量机/" title="支持向量机（SVM）"><div class="item__cover"><img src="/img/default_cover.png" alt="支持向量机（SVM）"></div><div class="item__info"><h3 class="item__title">支持向量机（SVM）</h3><span class="item__text">2018-01-27</span></div></a></li><li class="latest-post-item"><a href="/2017/08/26/2017-08-26-LogisticRegression/" title="Logistic回归"><div class="item__cover"><img src="/img/static/2017-08-26/output_34_0.png" alt="Logistic回归"></div><div class="item__info"><h3 class="item__title">Logistic回归</h3><span class="item__text">2017-08-26</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/hexo/">hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/matplotlib/">matplotlib</a></li><li class="tag-item"><a class="tag-link" href="/tags/教程/">教程</a></li><li class="tag-item"><a class="tag-link" href="/tags/机器学习/">机器学习</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站点基于 Hexo 搭建的静态资源博客，主要记录工作和生活中遇到的一些有趣的事物，仅用于交流，转载请著名出处，欢迎点击右下角订阅 rss。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Xi'an, Shaanxi Province, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>yangxiaoming512@yeah.net</span></li></ul></div></div><div class="footer-top__item footer__image"><img src="/img/qrcode.png" alt="logo" title="Gavinhome Blog"></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="https://github.com/Mrminfive/hexo-theme-skapp" title="hexo-theme-skapp" target="_blank">hexo-theme-skapp</a></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">构建工具</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="https://hexo.io/" title="Blog Framework" target="_blank">Hexo</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>.</p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/GavinHome" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="https://www.zhihu.com/people/jingliushui/activities" target="_blank" title="zhihu"><i class="iconfont icon-zhihu"></i></a></li><li class="social-network__item"><a href="https://weibo.com/u/6858743095" target="_blank" title="weibo"><i class="iconfont icon-weibo"></i></a></li><li class="social-network__item"><a href="mailto:yangxiaoming512@yeah.net" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li><li class="social-network__item"><a href="/atom.xml" target="_blank" title="rss"><i class="iconfont icon-rss"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="/js/md5.min.js"></script><script>var tags=["机器学习"],gitalk=new Gitalk({clientID:"9e9190730c0096e92ada",clientSecret:"241f5a5a27c8fed4af6f0e826b55df5f027724c2",repo:"gavinhome.github.io",owner:"GavinHome",admin:["GavinHome"],labels:tags,id:new Date(15180582e5).getTime()>new Date("2010-02-15").getTime()?md5(location.href):location.href});gitalk.render("comment-container")</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>